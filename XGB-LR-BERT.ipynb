{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8f7c7f-f99e-4f30-a418-0cec2722cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5655a755-4d85-490b-845e-c05a267ff5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Head:\n",
      "           id  label                                          statement  \\\n",
      "0  11972.json   true  Building a wall on the U.S.-Mexico border will...   \n",
      "1  11685.json  false  Wisconsin is on pace to double the number of l...   \n",
      "\n",
      "       subject            speaker             job_title state_info  \\\n",
      "0  immigration         rick-perry              Governor      Texas   \n",
      "1         jobs  katrina-shankland  State representative  Wisconsin   \n",
      "\n",
      "  party_affiliation  barely_true_counts  false_counts  half_true_counts  \\\n",
      "0        republican                  30            30                42   \n",
      "1          democrat                   2             1                 0   \n",
      "\n",
      "   mostly_true_counts  pants_on_fire_counts            context  target  \n",
      "0                  23                    18    Radio interview       1  \n",
      "1                   0                     0  a news conference       0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. LOAD DATA\n",
    "# ==========================================\n",
    "# Load the dataset found in your repository\n",
    "df = pd.read_csv('cleaned_liar_dataset.csv')\n",
    "\n",
    "print(\"Original Data Head:\")\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e1d7f5-251c-4e39-9015-c4a3c6b9e4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution (1=Real, 0=Fake):\n",
      "binary_label\n",
      "1    714\n",
      "0    553\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. PREPROCESS & LABEL MAPPING\n",
    "# ==========================================\n",
    "# CRITICAL STEP: Map the 6 labels to Binary (0 and 1)\n",
    "# This matches the logic likely used in 'baseline-LR-1.ipynb' to get high scores.\n",
    "\n",
    "def map_labels(label):\n",
    "    label = str(label).lower()\n",
    "    # Grouping 'True', 'Mostly True', and 'Half True' as REAL\n",
    "    if label in ['true', 'mostly-true', 'half-true']:\n",
    "        return 1\n",
    "    # Grouping 'False', 'Barely True', 'Pants Fire' as FAKE\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# NOTE: Verify 'label' matches the actual column name in your CSV. \n",
    "# If your column is named 'truth_rating', change 'label' below to 'truth_rating'.\n",
    "if 'label' in df.columns:\n",
    "    target_col = 'label'\n",
    "elif 'truth_rating' in df.columns:\n",
    "    target_col = 'truth_rating'\n",
    "else:\n",
    "    # Fallback: assume the second column is the label if names are missing\n",
    "    target_col = df.columns[4] \n",
    "    print(f\"Warning: Could not find standard label column. Using '{target_col}'\")\n",
    "\n",
    "# Apply mapping\n",
    "df['binary_label'] = df[target_col].apply(map_labels)\n",
    "\n",
    "# Verify the balance\n",
    "print(\"\\nLabel Distribution (1=Real, 0=Fake):\")\n",
    "print(df['binary_label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e8ce31-849f-4f2a-a329-3a6910a541c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading BERT model (this may take a moment)...\n",
      "Encoding text from column: 'statement'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc67205bbc448c0bd6d3097bbac64af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. BERT FEATURE EXTRACTION\n",
    "# ==========================================\n",
    "# Your project docs mention \"NLP techniques (BERT embeddings)\" [2]\n",
    "print(\"\\nLoading BERT model (this may take a moment)...\")\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# NOTE: Verify 'text' matches your actual text column name. \n",
    "# It might be named 'statement', 'tweet', or 'claim'.\n",
    "text_col = 'statement' if 'statement' in df.columns else df.columns # Fallback to 1st col\n",
    "\n",
    "print(f\"Encoding text from column: '{text_col}'...\")\n",
    "X_embeddings = bert_model.encode(df[text_col].astype(str).tolist(), show_progress_bar=True)\n",
    "y = df['binary_label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a31e26-a474-4d6c-a413-ec9fb19631e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 1013\n",
      "Testing set size: 254\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5. TRAIN/TEST SPLIT\n",
    "# ==========================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_embeddings, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e7c395-8c9b-436c-80bf-650bc339c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression ---\n",
      "Logistic Regression Accuracy: 0.5827\n",
      "Logistic Regression F1 Score: 0.5721\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6. MODEL 1: LOGISTIC REGRESSION (LR)\n",
    "# ==========================================\n",
    "# This replicates 'baseline-LR-1.ipynb'\n",
    "print(\"\\n--- Training Logistic Regression ---\")\n",
    "lr_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
    "print(f\"Logistic Regression F1 Score: {lr_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e377b131-2540-4c71-b0ed-99d753ccd871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training XGBoost ---\n",
      "XGBoost Accuracy: 0.5354\n",
      "XGBoost F1 Score: 0.5310\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 7. MODEL 2: XGBOOST\n",
    "# ==========================================\n",
    "# This replicates 'lr-xg-1.ipynb'\n",
    "print(\"\\n--- Training XGBoost ---\")\n",
    "xgb_model = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_f1 = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_acc:.4f}\")\n",
    "print(f\"XGBoost F1 Score: {xgb_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab8604c4-a626-46ee-8382-65792bf63ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL RESULTS ===\n",
      "LR Accuracy:  0.5827\n",
      "XGB Accuracy: 0.5354\n",
      "\n",
      "Recommendation: Use Logistic Regression for your final submission.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 8. FINAL COMPARISON\n",
    "# ==========================================\n",
    "print(\"\\n=== FINAL RESULTS ===\")\n",
    "print(f\"LR Accuracy:  {lr_acc:.4f}\")\n",
    "print(f\"XGB Accuracy: {xgb_acc:.4f}\")\n",
    "\n",
    "if xgb_acc > lr_acc:\n",
    "    print(\"\\nRecommendation: Use XGBoost for your final submission.\")\n",
    "else:\n",
    "    print(\"\\nRecommendation: Use Logistic Regression for your final submission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c3862-d3a2-4c01-86fc-8bf7e65a7960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
